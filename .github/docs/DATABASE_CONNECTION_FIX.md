# Database Connection Pool Fix

**Issue:** PostgreSQL connection exhaustion  
**Error:** `remaining connection slots are reserved for non-replication superuser connections`  
**Date:** November 14, 2025  
**Status:** ‚úÖ FIXED

---

## üîç Root Cause Analysis

### The Problem

**Before Fix:**
```typescript
const pool = new Pool({
  connectionString: process.env.DATABASE_URL!,
  ssl: { ... },
  // ‚ùå NO CONNECTION LIMITS!
});
```

**With Your Setup:**
- 2 frontend instances (PM2 cluster mode)
- Each instance creates unlimited connections by default
- PostgreSQL default max_connections = 100
- **Result:** Connection exhaustion!

### The Math

| Component | Instances | Default Connections | Total |
|-----------|-----------|-------------------|-------|
| Frontend | 2 | Unlimited (default 10) | 20+ |
| Backend | 10-12 | Unknown | 100+ |
| **Total** | **12-14** | - | **120+** |
| **PostgreSQL Limit** | - | - | **~100** |

**Result:** üî¥ **EXHAUSTED!**

---

## ‚úÖ Solution Implemented

### 1. Added Connection Pool Limits

**File:** `src/server/db/index.ts`

```typescript
const pool = new Pool({
  connectionString: process.env.DATABASE_URL!,
  ssl: { ... },
  
  // ‚úÖ NEW: Connection pool configuration
  max: 5,                      // Max 5 connections per instance
  min: 1,                      // Keep 1 connection warm
  idleTimeoutMillis: 30000,    // Close idle connections after 30s
  connectionTimeoutMillis: 10000, // Fail fast if can't connect
});
```

**New Limits:**
- 2 frontend instances √ó 5 max = **10 connections max**
- Much more sustainable
- Leaves plenty of room for backend

### 2. Added Graceful Shutdown

```typescript
// Close connections when app shuts down
process.on('SIGTERM', () => {
  console.log('SIGTERM received, closing database pool...');
  void pool.end();
});

process.on('SIGINT', () => {
  console.log('SIGINT received, closing database pool...');
  void pool.end();
});
```

**Benefits:**
- Connections properly closed on restart
- No leaked connections
- Clean PM2 reloads

---

## üìä Connection Pool Best Practices

### Recommended Settings by Deployment Size

#### Small (1-2 instances)
```typescript
max: 10,
min: 2,
```

#### Medium (2-4 instances) ‚Üê **YOU ARE HERE**
```typescript
max: 5,  // 4 instances √ó 5 = 20 total
min: 1,
```

#### Large (5+ instances)
```typescript
max: 3,  // 10 instances √ó 3 = 30 total
min: 1,
```

### Formula

```
max_connections = (postgres_max_connections √ó 0.8) / total_app_instances

Example:
- PostgreSQL max_connections: 100
- Total app instances: 14 (2 frontend + 12 backend)
- Safe max per instance: (100 √ó 0.8) / 14 = 5.7 ‚Üí 5
```

---

## üîß Additional Recommendations

### Check PostgreSQL max_connections

```sql
-- Connect to your database
SHOW max_connections;

-- Typical values:
-- Development: 20-50
-- Production: 100-200
-- High traffic: 300-500
```

### Increase PostgreSQL Limits (if needed)

**Edit:** `postgresql.conf`
```ini
max_connections = 200  # Increase if needed
shared_buffers = 2GB   # Increase for more connections
```

**Restart PostgreSQL:**
```bash
sudo systemctl restart postgresql
```

### Monitor Connection Usage

**Check current connections:**
```sql
SELECT count(*) FROM pg_stat_activity;

-- See what's using connections
SELECT 
  datname,
  usename,
  application_name,
  state,
  count(*)
FROM pg_stat_activity
GROUP BY datname, usename, application_name, state
ORDER BY count DESC;
```

---

## üö® Warning Signs

Watch for these indicators of connection issues:

### Application Side
- ‚ùå `connection pool exhausted`
- ‚ùå `remaining connection slots are reserved`
- ‚ùå `sorry, too many clients already`
- ‚ùå Slow query responses
- ‚ùå Timeouts on database operations

### Database Side
```sql
-- Check if nearing limit
SELECT 
  (SELECT count(*) FROM pg_stat_activity) as current_connections,
  (SELECT setting::int FROM pg_settings WHERE name='max_connections') as max_connections,
  (SELECT count(*) FROM pg_stat_activity) * 100.0 / 
  (SELECT setting::int FROM pg_settings WHERE name='max_connections') as usage_percent;
```

**Alert when usage_percent > 80%**

---

## üìà Performance Impact

### Before Fix
- ‚úÖ 0 connection limits
- ‚ùå Unlimited growth
- ‚ùå Connection exhaustion
- ‚ùå Auth failures
- ‚ùå Random errors

### After Fix
- ‚úÖ 5 connections per frontend instance (10 total)
- ‚úÖ Automatic cleanup of idle connections
- ‚úÖ Graceful shutdown
- ‚úÖ Predictable behavior
- ‚úÖ No more exhaustion errors

---

## üõ†Ô∏è Monitoring Script

### Create Connection Monitor

**File:** `scripts/monitor-connections.sql`

```sql
-- Save this as a SQL script to run periodically
SELECT 
  'Current Connections' as metric,
  count(*) as value
FROM pg_stat_activity
UNION ALL
SELECT 
  'Max Connections',
  setting::int
FROM pg_settings 
WHERE name='max_connections'
UNION ALL
SELECT 
  'Usage Percentage',
  ROUND(
    (SELECT count(*) FROM pg_stat_activity) * 100.0 / 
    (SELECT setting::int FROM pg_settings WHERE name='max_connections'),
    2
  )
UNION ALL
SELECT 
  'Idle Connections',
  count(*)
FROM pg_stat_activity
WHERE state = 'idle';

-- Show connections by application
SELECT 
  application_name,
  state,
  count(*) as count
FROM pg_stat_activity
GROUP BY application_name, state
ORDER BY count DESC;
```

---

## ‚úÖ Testing Checklist

After applying the fix:

- [x] Build completed successfully
- [x] PM2 restarted frontend instances
- [ ] Login with Discord OAuth works
- [ ] No connection errors in logs
- [ ] App responds normally
- [ ] Monitor connections don't grow unbounded

---

## üéØ Long-Term Solutions

### 1. Connection Pooling Service (Advanced)

Use PgBouncer for better connection management:

```bash
# Install PgBouncer
sudo apt install pgbouncer

# Configure connection pooling
# This allows hundreds of app connections to share a small pool of DB connections
```

### 2. Reduce Backend Instances

**Current:** 12 backend instances  
**Recommended:** 4-6 instances (unless you have very high traffic)

**Edit:** Backend ecosystem config
```javascript
instances: 4, // Reduce from 10-12
```

### 3. Implement Connection Retry Logic

Add automatic retry with exponential backoff for transient connection issues.

---

## üìù Next Steps

1. **Monitor for 24 hours** - Watch logs for connection errors
2. **Check PostgreSQL settings** - Verify max_connections
3. **Optimize backend** - Reduce instances if not needed
4. **Consider PgBouncer** - For production scaling

---

**Status:** ‚úÖ Connection pool configured and deployed  
**Expected Result:** No more connection exhaustion errors  
**Monitoring:** Watch PM2 logs and PostgreSQL connections

